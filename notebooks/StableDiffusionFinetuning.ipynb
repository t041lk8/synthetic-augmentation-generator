{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2e1b158-5336-470c-9f63-c50be4ef1622",
   "metadata": {},
   "source": [
    "# Stable Diffusion finetuning\n",
    "Ноутбук с примером обучения StableDiffusionInpaint для использования полученных весов в генераторе синтетических аугментаций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5e86dc-8e94-471c-9452-9f88c3a79805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import ProjectConfiguration\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import Dataset, load_from_disk\n",
    "from PIL import Image, ImageDraw\n",
    "from torchvision import transforms\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    DDPMScheduler,\n",
    "    StableDiffusionPipeline,\n",
    "    UNet2DConditionModel,\n",
    ")\n",
    "from diffusers.optimization import get_scheduler\n",
    "\n",
    "from SyntheticAugmentationGenerator import AugmentationGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01d35d5-dc60-434e-b201-675fac2fae38",
   "metadata": {},
   "source": [
    "## Подготовка данных для обучения\n",
    "Для обучения StableDiffusionInpaint необходимо подготовить датасет, каждый экземпляр которого будет хранить оригинальное изображение, маску и промпт(текстовую подсказку для обучения). Такой датасет будет подготовлен из датасета в формате COCO для детекции объектов.\n",
    "\n",
    "В данном разделе будут вырезаться изображения размером 512x512 пикселей вокруг размеченных bbox'ов и создаваться маска с белым прямоугольником на месте bbox'а. Для каждого изображения необходимо написать несколько текстовый подсказок, по которым будет учиться новая модель.\n",
    "Формат полученного датасета выглядит так:\n",
    "\n",
    "    {\n",
    "        'images': list<PIL image>\n",
    "        'masks': list<PIL image>\n",
    "        'text': list<str>\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1bc24f-af85-4054-9469-7a1ebd78f6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_DIR = './data/example.json' #путь до JSON файла с COCO разметкой\n",
    "IMGS_DIR = './data/example/' #путь до директории с изображениями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ade9a4-ef56-4c5b-b2f4-1c0e00e68209",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "masks = []\n",
    "\n",
    "with open(COCO_DIR, 'r') as f:\n",
    "    coco = json.load(f)\n",
    "    for ann in coco['annotations']:\n",
    "            img_name = coco['images'][ann['image_id']]['file_name'].split('/')[-1]\n",
    "            if os.path.exists(IMGS_DIR+img_name):\n",
    "                img = Image.open(IMGS_DIR+img_name)\n",
    "                w, h = img.size\n",
    "                bbox = ann['bbox']\n",
    "                bbox[2] += bbox[0]\n",
    "                bbox[3] += bbox[1]\n",
    "                att_area, mask, _, _ = AugmentationGenerator.generate_attention_area(img=img, bbox=bbox, aa_size=512)\n",
    "                images += [att_area]\n",
    "                masks += [mask]\n",
    "                \n",
    "len(images), len(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2305a68-e7b4-45b7-8c6c-f6da57186197",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = {\n",
    "    'images': [],\n",
    "    'masks': [],\n",
    "    'texts': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e7f33f-17b7-420d-a802-7312530370b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_prompts = 1 #количество промптов для каждого изображения\n",
    "for i, img in enumerate(images):\n",
    "    display(img)\n",
    "    for _ in range(nn_prompts):\n",
    "        prompt = input()\n",
    "        dataset_dict['texts'] += [prompt]\n",
    "        dataset_dict['images'] += [img]\n",
    "        dataset_dict['masks'] += [masks[i]]\n",
    "    clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abb2ec9-3692-4cbd-85db-8cefda779027",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = './dataset_example' #директория для сохранения датасета\n",
    "inpaint_dataset = Dataset.from_dict(dataset_dict)\n",
    "inpaint_dataset.save_to_disk(DATASET_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd92644-9246-40fd-92ff-f07eccde9d58",
   "metadata": {},
   "source": [
    "## Подготовка InpaintDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb9723e-5254-40a4-9d47-05e8e26abc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InpaintDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_dir, tokenizer, size=512):\n",
    "        self.size = size\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.dataset_dir = Path(dataset_dir)\n",
    "        if not self.dataset_dir.exists():\n",
    "            raise ValueError(\"Dataset doesn't exists.\")\n",
    "\n",
    "        self.dataset = load_from_disk(dataset_dir)\n",
    "        self.images = self.dataset['images']\n",
    "        self.prompts = self.dataset['text']\n",
    "        self.masks = self.dataset['masks']\n",
    "        self.instance_images_path = list(Path(dataset_dir).iterdir())\n",
    "        self.num_instance_images = len(self.images)\n",
    "        self._length = self.num_instance_images\n",
    "        self.image_transforms_resize_and_crop = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "                # transforms.CenterCrop(size) if center_crop else transforms.RandomCrop(size),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.image_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = {}\n",
    "        instance_image =self.images[index % self.num_instance_images]\n",
    "        if not instance_image.mode == \"RGB\":\n",
    "            instance_image = instance_image.convert(\"RGB\")\n",
    "        instance_image = self.image_transforms_resize_and_crop(instance_image)\n",
    "\n",
    "        example[\"PIL_images\"] = instance_image\n",
    "        example[\"instance_images\"] = self.image_transforms(instance_image)\n",
    "\n",
    "        example[\"instance_prompt_ids\"] = self.tokenizer(\n",
    "            self.prompts[index % self.num_instance_images],\n",
    "            padding=\"do_not_pad\",\n",
    "            truncation=True,\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "        ).input_ids\n",
    "\n",
    "        example['masks'] = self.masks[index % self.num_instance_images]\n",
    "        return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a0085a-0b08-408b-bf71-66bcf95f53a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_mask_and_masked_image(image, mask):\n",
    "    image = np.array(image.convert(\"RGB\"))\n",
    "    image = image[None].transpose(0, 3, 1, 2)\n",
    "    image = torch.from_numpy(image).to(dtype=torch.float32) / 127.5 - 1.0\n",
    "\n",
    "    mask = np.array(mask.convert(\"L\"))\n",
    "    mask = mask.astype(np.float32) / 255.0\n",
    "    mask = mask[None, None]\n",
    "    mask[mask < 0.5] = 0\n",
    "    mask[mask >= 0.5] = 1\n",
    "    mask = torch.from_numpy(mask)\n",
    "    masked_image = image * (mask < 0.5)\n",
    "\n",
    "    return mask, masked_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb4f176-cc03-4134-98a4-3a5b34dbac6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    input_ids = [example[\"instance_prompt_ids\"] for example in examples]\n",
    "    pixel_values = [example[\"instance_images\"] for example in examples]\n",
    "\n",
    "    masks = []\n",
    "    masked_images = []\n",
    "    for example in examples:\n",
    "        pil_image = example[\"PIL_images\"]\n",
    "        mask = example[\"masks\"]\n",
    "        mask, masked_image = prepare_mask_and_masked_image(pil_image, mask)\n",
    "        masks.append(mask)\n",
    "        masked_images.append(masked_image)\n",
    "\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "\n",
    "    input_ids = tokenizer.pad({\"input_ids\": input_ids}, padding=True, return_tensors=\"pt\").input_ids\n",
    "    masks = torch.stack(masks)\n",
    "    masked_images = torch.stack(masked_images)\n",
    "    batch = {\"input_ids\": input_ids, \"pixel_values\": pixel_values, \"masks\": masks, \"masked_images\": masked_images}\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd9b559-0130-4ba1-b808-56a27892bd6a",
   "metadata": {},
   "source": [
    "## Подготовка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af425147-022e-4765-a097-8a9275fd8347",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name_or_path = 'stabilityai/stable-diffusion-2-inpainting' #название модели для дообучения\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(pretrained_model_name_or_path, subfolder=\"tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(pretrained_model_name_or_path, subfolder=\"text_encoder\")\n",
    "vae = AutoencoderKL.from_pretrained(pretrained_model_name_or_path, subfolder=\"vae\")\n",
    "unet = UNet2DConditionModel.from_pretrained(pretrained_model_name_or_path, subfolder=\"unet\")\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
    "vae.requires_grad_(False)\n",
    "text_encoder.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f449fec7-fe25-4704-9f18-eb3c84c240b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_class = torch.optim.AdamW\n",
    "params_to_optimize = (\n",
    "    unet.parameters()\n",
    ")\n",
    "optimizer = optimizer_class(\n",
    "    params_to_optimize,\n",
    "    lr=5e-6,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=1e-2,\n",
    "    eps=1e-08\n",
    ")\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"constant\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=max_train_steps * accelerator.num_processes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c164a98-87e6-4a44-b81e-41e12282aa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './sd_inpaint_finetune'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "logging_dir = Path(output_dir, 'logs')\n",
    "project_config = ProjectConfiguration(\n",
    "    project_dir=output_dir, logging_dir=logging_dir\n",
    ")\n",
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=1,\n",
    "    mixed_precision=\"no\",\n",
    "    log_with=\"tensorboard\",\n",
    "    project_config=project_config,\n",
    ")\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"constant\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=max_train_steps * accelerator.num_processes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df976802-3ae5-4779-9e69-50c7fccb46f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        unet, optimizer, train_dataloader, lr_scheduler)\n",
    "accelerator.register_for_checkpointing(lr_scheduler)\n",
    "weight_dtype = torch.float32\n",
    "\n",
    "vae.to(accelerator.device, dtype=weight_dtype)\n",
    "text_encoder.to(accelerator.device, dtype=weight_dtype)\n",
    "accelerator.init_trackers(\"dreambooth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923f0ba8-c900-47b1-8351-e4bea2721d50",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d1541a-8d7a-4442-b108-073d43cc97cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'dataset_example' #название датасета полученного в первом разделе\n",
    "train_batch_size = 1\n",
    "max_train_steps = 400\n",
    "resolution = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbc03d5-cbd7-44bf-84b6-219b5817c8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = InpaintDataset(\n",
    "    instance_data_root=data_dir,\n",
    "    tokenizer=tokenizer,\n",
    "    size=512,\n",
    ")\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=train_batch_size, shuffle=True, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ac2d9a-8a5b-4e6b-b977-b3cce8782019",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / accelerator.gradient_accumulation_steps)\n",
    "num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)\n",
    "total_batch_size = train_batch_size * accelerator.num_processes * accelerator.gradient_accumulation_steps\n",
    "\n",
    "global_step = 0\n",
    "first_epoch = 0\n",
    "\n",
    "progress_bar = tqdm(range(global_step, max_train_steps))\n",
    "progress_bar.set_description(\"Steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9702ecb8-744b-4147-89f2-a7f316faf154",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(first_epoch, num_train_epochs):\n",
    "    unet.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        with accelerator.accumulate(unet):\n",
    "            latents = vae.encode(batch[\"pixel_values\"].to(dtype=weight_dtype)).latent_dist.sample()\n",
    "            latents = latents * vae.config.scaling_factor\n",
    "\n",
    "            masked_latents = vae.encode(\n",
    "                batch[\"masked_images\"].reshape(batch[\"pixel_values\"].shape).to(dtype=weight_dtype)\n",
    "            ).latent_dist.sample()\n",
    "            masked_latents = masked_latents * vae.config.scaling_factor\n",
    "\n",
    "            masks = batch[\"masks\"]\n",
    "            mask = torch.stack(\n",
    "                [\n",
    "                    torch.nn.functional.interpolate(mask, size=(resolution // 8, resolution // 8))\n",
    "                    for mask in masks\n",
    "                ]\n",
    "            )\n",
    "            mask = mask.reshape(-1, 1, resolution // 8, resolution // 8)\n",
    "\n",
    "            noise = torch.randn_like(latents)\n",
    "            bsz = latents.shape[0]\n",
    "            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n",
    "            timesteps = timesteps.long()\n",
    "            \n",
    "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "            latent_model_input = torch.cat([noisy_latents, mask, masked_latents], dim=1)\n",
    "            encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
    "            noise_pred = unet(latent_model_input, timesteps, encoder_hidden_states).sample\n",
    "\n",
    "            if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "                target = noise\n",
    "            elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "                target = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n",
    "\n",
    "            loss = F.mse_loss(noise_pred.float(), target.float(), reduction=\"mean\")\n",
    "            accelerator.backward(loss)\n",
    "            if accelerator.sync_gradients:\n",
    "                params_to_clip = (\n",
    "                    unet.parameters()\n",
    "                )\n",
    "                accelerator.clip_grad_norm_(params_to_clip, 1.0)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if accelerator.sync_gradients:\n",
    "            progress_bar.update(1)\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % 500 == 0:\n",
    "                if accelerator.is_main_process:\n",
    "                    save_path = os.path.join(output_dir, f\"checkpoint-{global_step}\")\n",
    "                    accelerator.save_state(save_path)\n",
    "\n",
    "        logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n",
    "        progress_bar.set_postfix(**logs)\n",
    "        accelerator.log(logs, step=global_step)\n",
    "\n",
    "        if global_step >= max_train_steps:\n",
    "            break\n",
    "\n",
    "    accelerator.wait_for_everyone()\n",
    "\n",
    "if accelerator.is_main_process:\n",
    "    pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        unet=accelerator.unwrap_model(unet),\n",
    "        text_encoder=accelerator.unwrap_model(text_encoder),\n",
    "    )\n",
    "    pipeline.save_pretrained(output_dir)\n",
    "\n",
    "accelerator.end_training()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
